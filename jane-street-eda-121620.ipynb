{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# <font color='darkblue'>Jane Street Market Prediction</font>\n### This is an extensive data analysis for the jane street market dataset, this notebook will go through the train and features csv's for an extensive exploratory data analysis, Also some data cleaning and preprocessing will be done along the way.\n#### Please note that this is a relatively large notebook with a lot of exhaustive analytics as it meant to be that way to give a comprehensive understanding of the dataset so it might take a considerable amount of time to run ( around 30 minutes) so be patient.\n## [The EDA](#eda) will be devided to two main parts:\n      \n## 1- [General EDA](#general)\n>#### **Which will include the following:**\n  >- [Resp Data Analysis](#resp)\n  >- [Date](#date)\n  >- [Weight](#weight)\n\n## 2 - [Features Data Analysis](#features)\n>#### **Which will include the following:**\n  >- [Null Values](#nulls)\n  >- [Cumulative growth](#growth)\n  >- [Multicollinearity](#multicollinearity)\n  >- [Outliers](#outlier)\n  >- [Feature 0](#f0)"},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Let's dive right in!</center></h1>\n\n![](https://media1.tenor.com/images/ed3ccde29b0efef4a88e13353f6923ba/tenor.gif)\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport sys\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\nfrom IPython.core.display import display, HTML\nsns.set_style('darkgrid')\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n# from sklearn.linear_model import LogisticRegression as log\n# from sklearn.model_selection import train_test_split as split \n# from sklearn.model_selection import GridSearchCV as Grid\n# from sklearn.naive_bayes import GaussianNB as GNB\n# from sklearn.svm import SVC\n# from sklearn.linear_model import LinearRegression as linear\n# from sklearn.linear_model import SGDClassifier as SGD\n# import xgboost as xgb\n# from sklearn.decomposition import IncrementalPCA as ipca\n# from sklearn.metrics import (roc_auc_score, precision_score, recall_score, f1_score,\n#                              confusion_matrix, accuracy_score, roc_curve, auc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install plotly --upgrade","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Loading Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jane-street-market-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 140)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving the original dataset length\norg_len = len(df)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Trying to cut corners to save some memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.feature_0 = df.feature_0.astype(np.int8)\ndf.date= df.date.astype(np.int16)\ndf.ts_id = df.ts_id.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The following code loop through the dataframe to change float64 columns to float32 only if there is a really few amount of data (a very conservative threshold of 0.1%) between -.0001:.0001 to avoid hurting accuracy of small values columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df:\n    if df[i].dtype == np.float64:\n        if (((df[i] < .0001) & (df[i] > -.0001)).mean()) > .001:\n            print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df:\n    if df[i].dtype == np.float64:\n        if (((df[i] < .0001) & (df[i] > -.0001)).mean()) < .001:\n            df[i] = df[i].astype(np.float32)\n            gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### we can see a noticeable difference in memory usage"},{"metadata":{},"cell_type":"markdown","source":"### Assuring that Data is stored by date"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by= ['date','ts_id'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding target"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['action'] = np.where(df['resp'] > 0,1,0)\ndf.action = df.action.astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda\"></a>\n# **EDA**"},{"metadata":{},"cell_type":"markdown","source":"<a id='general'></a>\n# **General Data Analysis**"},{"metadata":{},"cell_type":"markdown","source":"<a id='resp'></a>\n## **Resp Data Analysis** "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,6))\nax = plt.subplot(1,1,1)\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']].sum().cumsum().plot(ax=ax)\nplt.title('Cumulative Sum of Different RESP\\'s',fontsize=18)\nplt.xlabel('Date',fontsize=14)\nplt.legend(fontsize=12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It can be noticed the there were more gain in the first 100 days, We can also notice that resp_4 has the highest cumulative sum on the other hand resp_1 has the smallest cumulative sum."},{"metadata":{},"cell_type":"markdown","source":"### Now we plot the average of each Resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean(),\n              x= df.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean().index,\n              y= ['resp_1', 'resp_2', 'resp_3', 'resp_4','resp'],\n              title= '\\naverage Resp per day')\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Avg Resp'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,((ax11,ax12,ax13),(ax21,ax22,ax23),(ax31,ax32,ax33),(ax41,ax42,ax43),(ax51,ax52,ax53)) = plt.subplots(5,3,figsize=(20,24))\nplt.subplots_adjust(hspace=0.35)\nax11.hist(df.resp,bins=150)\nax11.axvline(df.resp.mean()+df.resp.std(),color='darkorange',alpha=.8)\nax11.axvline(df.resp.mean()-df.resp.std(),color='darkorange',alpha=.8)\ndf.resp.plot.hist(bins= 150,ax=ax12)\nax12.axvline(df.resp.mean()+df.resp.std(),color='darkorange',alpha=.8)\nax12.axvline(df.resp.mean()-df.resp.std(),color='darkorange',alpha=.8)\nax12.set_xlim(-.08,.08)\nax13.hist(df.resp,bins=150)\nax13.set_yscale('log')\nskew= round(df.resp.skew(),4)\nkurt= round(df.resp.kurtosis())\nstd1= round((((df.resp.mean()-df.resp.std()) < df.resp ) & (df.resp < (df.resp.mean()+df.resp.std()))).mean()*100,2)\nprops = dict(boxstyle='round', facecolor='white', alpha=0.5)\nax11.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp.mean(),4),round(df.resp.std(),4),skew,kurt,std1),\n         transform=ax11.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax11.set_title('Resp Hist Normal scale',fontsize=14)\nax12.set_title('Resp Hist normal scale zoomed',fontsize=14)\nax13.set_title('Resp Hist with freq on a log scale',fontsize=14);\nax11.set_xlabel('')\nax11.set_ylabel('')\nax12.set_xlabel('')\nax12.set_ylabel('')\nax13.set_xlabel('')\nax13.set_ylabel('')\nax21.hist(df.resp_1,bins=150)\nax21.axvline(df.resp_1.mean()+df.resp_1.std(),color='darkorange',alpha=.8)\nax21.axvline(df.resp_1.mean()-df.resp_1.std(),color='darkorange',alpha=.8)\ndf.resp_1.plot.hist(bins= 150,ax=ax22)\nax22.axvline(df.resp_1.mean()+df.resp_1.std(),color='darkorange',alpha=.8)\nax22.axvline(df.resp_1.mean()-df.resp_1.std(),color='darkorange',alpha=.8)\nax22.set_xlim(-.08,.08)\nax23.hist(df.resp_1,bins=150)\nax23.set_yscale('log')\nskew= round(df.resp_1.skew(),4)\nkurt= round(df.resp_1.kurtosis())\nstd1= round((((df.resp_1.mean()-df.resp_1.std()) < df.resp_1 ) & (df.resp_1 < (df.resp_1.mean()+df.resp_1.std()))).mean()*100,2)\nax21.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_1.mean(),4),round(df.resp_1.std(),4),skew,kurt,std1),\n         transform=ax21.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax21.set_title('Resp_1 Hist Normal scale',fontsize=14)\nax22.set_title('Resp_1 Hist normal scale zoomed',fontsize=14)\nax23.set_title('Resp_1 Hist with freq on a log scale',fontsize=14);\nax21.set_xlabel('')\nax21.set_ylabel('')\nax22.set_xlabel('')\nax22.set_ylabel('')\nax23.set_xlabel('')\nax23.set_ylabel('')\nax31.hist(df.resp_2,bins=150)\nax31.axvline(df.resp_2.mean()+df.resp_2.std(),color='darkorange',alpha=.8)\nax31.axvline(df.resp_2.mean()-df.resp_2.std(),color='darkorange',alpha=.8)\ndf.resp_2.plot.hist(bins= 150,ax=ax32)\nax32.axvline(df.resp_2.mean()+df.resp_2.std(),color='darkorange',alpha=.8)\nax32.axvline(df.resp_2.mean()-df.resp_2.std(),color='darkorange',alpha=.8)\nax32.set_xlim(-.08,.08)\nax33.hist(df.resp_2,bins=150)\nax33.set_yscale('log')\nskew= round(df.resp_2.skew(),4)\nkurt= round(df.resp_2.kurtosis())\nstd1= round((((df.resp_2.mean()-df.resp_2.std()) < df.resp_2 ) & (df.resp_2 < (df.resp_2.mean()+df.resp_2.std()))).mean()*100,2)\nax31.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_2.mean(),4),round(df.resp_2.std(),4),skew,kurt,std1),\n         transform=ax31.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax31.set_title('Resp_2 Hist Normal scale',fontsize=14)\nax32.set_title('Resp_2 Hist normal scale zoomed',fontsize=14)\nax33.set_title('Resp_2 Hist with freq on a log scale',fontsize=14);\nax31.set_xlabel('')\nax31.set_ylabel('')\nax32.set_xlabel('')\nax32.set_ylabel('')\nax33.set_xlabel('')\nax33.set_ylabel('')\nax41.hist(df.resp_3,bins=150)\nax41.axvline(df.resp_3.mean()+df.resp_3.std(),color='darkorange',alpha=.8)\nax41.axvline(df.resp_3.mean()-df.resp_3.std(),color='darkorange',alpha=.8)\ndf.resp_3.plot.hist(bins= 150,ax=ax42)\nax42.axvline(df.resp_3.mean()+df.resp_3.std(),color='darkorange',alpha=.8)\nax42.axvline(df.resp_3.mean()-df.resp_3.std(),color='darkorange',alpha=.8)\nax42.set_xlim(-.08,.08)\nax43.hist(df.resp_3,bins=150)\nax43.set_yscale('log')\nskew= round(df.resp_3.skew(),4)\nkurt= round(df.resp_3.kurtosis())\nstd1= round((((df.resp_3.mean()-df.resp_3.std()) < df.resp_3 ) & (df.resp_3 < (df.resp_3.mean()+df.resp_3.std()))).mean()*100,2)\nax41.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_3.mean(),4),round(df.resp_3.std(),4),skew,kurt,std1),\n         transform=ax41.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax41.set_title('Resp_3 Hist Normal scale',fontsize=14)\nax42.set_title('Resp_3 Hist normal scale zoomed',fontsize=14)\nax43.set_title('Resp_3 Hist with freq on a log scale',fontsize=14);\nax41.set_xlabel('')\nax41.set_ylabel('')\nax42.set_xlabel('')\nax42.set_ylabel('')\nax43.set_xlabel('')\nax43.set_ylabel('')\nax51.hist(df.resp_4,bins=150)\nax51.axvline(df.resp_4.mean()+df.resp_4.std(),color='darkorange',alpha=.8)\nax51.axvline(df.resp_4.mean()-df.resp_4.std(),color='darkorange',alpha=.8)\ndf.resp_4.plot.hist(bins= 150,ax=ax52)\nax52.axvline(df.resp_4.mean()+df.resp_4.std(),color='darkorange',alpha=.8)\nax52.axvline(df.resp_4.mean()-df.resp_4.std(),color='darkorange',alpha=.8)\nax52.set_xlim(-.08,.08)\nax53.hist(df.resp_4,bins=150)\nax53.set_yscale('log')\nskew= round(df.resp_4.skew(),4)\nkurt= round(df.resp_4.kurtosis())\nstd1= round((((df.resp_4.mean()-df.resp_4.std()) < df.resp_4 ) & (df.resp_4 < (df.resp_4.mean()+df.resp_4.std()))).mean()*100,2)\nax51.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_4.mean(),4),round(df.resp_4.std(),4),skew,kurt,std1),\n         transform=ax51.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax51.set_title('Resp_4 Hist Normal scale',fontsize=14)\nax52.set_title('Resp_4 Hist normal scale zoomed',fontsize=14)\nax53.set_title('Resp_4 Hist with freq on a log scale',fontsize=14)\nax51.set_xlabel('')\nax51.set_ylabel('')\nax52.set_xlabel('')\nax52.set_ylabel('')\nax53.set_xlabel('')\nax53.set_ylabel('')\nfig.suptitle('RESPs Historgrams on Different Scales',fontsize=18,y=.92);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']],corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that Resp is highly related to Resp_4\nalso Resp_1 and Resp_2 are highly relted to each other \nfrom the relation shown in this figure and the standard deviation and distribution shown in the figure before this one we can assume that Resp is more related to longer time horizon invest as longer time horizon are associated with more return and higher risk\n\n### [The Basics of Investment Time Horizons](https://www.investopedia.com/terms/t/timehorizon.asp)\n> ####  An Investment Time Horizon is the period where one expects to hold an investment for a specific goal. Investments are generally broken down into two main categories: stocks (riskier) and bonds (less risky). The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt. \n\n[source: investopedia.com](https://www.investopedia.com/terms/t/timehorizon.asp)"},{"metadata":{},"cell_type":"markdown","source":"<a id='date'></a>\n## **Date**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.date.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### The Date seem to contain 2 years of trading data since the trading days of the year are approximately 252 : 253 days\nhttps://en.wikipedia.org/wiki/Trading_day"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.area(data_frame= df.groupby('date')[['resp']].count(),title='Number of operation per day')\nfig.update_traces( showlegend = False)\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Number of operations'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will create a moving standard deviation of 20 days (which is a month of trading) for the average of resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_df = df.groupby('date')[['resp']].mean()\nstd20 = []\nfor i in range(len(date_df)):\n    if i <20:\n        std20.append(np.nan)\n    else:\n        moving_std = date_df['resp'][i-20:i].std()\n        std20.append(moving_std)\ndate_df['moving_std'] = std20\ndate_df.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(data_frame=date_df,y=['resp','moving_std'],title='Average Resp & 20 day moving standard deviation')\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Avg Resp'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we check the standard deviation of each resp for each day"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(14,12))\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4']].std().plot(ax=ax1,color=['steelblue','darkorange','red','green'],alpha=.8)\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4']].std().plot.kde(ax=ax2)\nfig.suptitle('Resp\\'s Std',fontsize=18,y=.96)\nax2.set_xlabel('')\nax1.set_xlabel('')\nax2.set_title('kde of each resp std', fontsize=14)\nax1.set_title('std of Resp\\'s for each trading day',fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### As it was mentioned before the standard deviation seems to increase with resp mostly related to longer time horizon investments\n> ### It can also be noticed that the deviation was kinda higher in the first 100 days as it was mentioned by many kagglers that there may was some kind of trading model adjustment done after the 80th day."},{"metadata":{},"cell_type":"markdown","source":"<a id='weight'></a>\n## **Weight**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.weight[df.weight !=0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,7))\ngrid =  gridspec.GridSpec(2,3,figure=fig,hspace=.3,wspace=.2)\nax1 = fig.add_subplot(grid[0, 0])\nax2 = fig.add_subplot(grid[0, 1])\nax3 = fig.add_subplot(grid[1, 0])\nax4 = fig.add_subplot(grid[1, 1])\nax5 = fig.add_subplot(grid[:, 2])\nsns.boxplot(x = df.weight,width=.5,ax=ax1)\nax2.hist(df.weight, color='#404788ff',alpha=.6, bins= list([-.05] + list(10**np.arange(-2,2.24,.05))))\nax2.set_xscale('symlog')\nax2.set_xlim(-.05,227)\nsns.boxplot(x = df.weight[df.weight != 0],width=.5,ax=ax3)\nax1.set_title('Weights including zero weights',fontsize=14)\nax3.set_title('Weights not including zero weights',fontsize=14)\nax2.set_title('Weights including zero weights (log)',fontsize=14)\nax4.set_title('Weights not including zero weights (log)',fontsize=14)\nprops = dict(boxstyle='round', facecolor='white', alpha=0.4)\nax1.text(.2,.9,'μ = {}    std = {}\\nmin = {}    max = {}'.format(round(df.weight.mean(),3),round(df.weight.std(),3),round(df.weight.min(),3),round(df.weight.max(),3)),\n         transform=ax1.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax3.text(.2,.9,'μ = {}        std = {}\\nmin = {}    max = {}'.format(round(df.weight[df.weight != 0].mean(),3),round(df.weight[df.weight != 0].std(),3),\n                                                              round(df.weight[df.weight != 0].min(),3),round(df.weight[df.weight != 0].max(),3)),\n         transform=ax3.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax4.hist(df.weight[df.weight !=0],color='#404788ff',alpha=.6,bins=10**np.arange(-2.16,2.24,.05))\nax4.set_xscale('log')\nax4.set_xticks((.01,.03,.1,.3,1,3,10,30,100))\nax4.set_xticklabels((.01,.03,.1,.3,1,3,10,30,100))\nax5.pie(((df.weight==0).mean(),(1-(df.weight==0).mean())),startangle=300,wedgeprops=dict(width=0.5),\n        labels=('Zeros\\n{}%'.format(round((df.weight==0).mean()*100,2)),'Nonzeros\\n{}%'.format(round((1-(df.weight==0).mean())*100,2))),\n        textprops={'fontsize': 12},colors=['#404788ff','#55c667ff'])\nax5.set_title('Zeros vs non-zero weights',fontsize=14)\nax1.set_xlabel('')\nax2.set_xlabel('')\nax3.set_xlabel('')\nax2.set_ylabel('')\nax5.set_ylabel('')\nax4.set_xlabel('');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nfig.suptitle('Nonzero weights histogram in different scales',fontsize=18)\nax1 = plt.subplot(3,1,1)\nax1.hist(df.weight[df.weight !=0],color='darkblue',alpha=.7, bins=10**np.arange(-2.16,2.23,.05))\nplt.xscale('log')\nplt.xticks((.01,.03,.1,.3,1,3,10,30,100),(.01,.03,.1,.3,1,3,10,30,100))\nax2 = plt.subplot(3,1,2)\nsns.distplot(df.weight[df.weight != 0], color='darkblue', bins=400, ax=ax2) \nax3 = plt.subplot(3,1,3)\nax3.hist(df.weight[(df.weight !=0) & (df.weight < 3.197 )],color='darkblue',alpha=.7, bins=200)\nax3.set_xlim(0,3.3)\nax2.set_xlabel('') \nax1.set_title('All values (log-scale)',fontsize=14)\nax2.set_title('kde of the distribution',fontsize=14)\nax3.set_title('75% of the Values',fontsize=14)\nplt.subplots_adjust(hspace=.4);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we take a deeper look at outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(16,8))\nfig.suptitle('Weight outliers',fontsize=18)\nsns.boxplot(df.weight,width=.5, ax=ax1)\nax1.axvline(np.percentile(df.weight,95), color= 'green',label='95.0%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99), color= 'darkblue',label='99.0%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99.9), color= 'darkorange',label='99.9%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99.99), color= 'magenta',label='99.99%',linestyle=':',linewidth=3)\nax1.legend(fontsize=13)\nsns.boxplot(df.weight[df.weight !=0],width=.5, ax=ax2)\nax2.axvline(np.percentile(df.weight[df.weight !=0],95), color= 'green',label='95.0%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99), color= 'darkblue',label='99.0%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99.9), color= 'darkorange',label='99.9%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99.99), color= 'magenta',label='99.99%',linestyle=':',linewidth=3)\nax2.legend(fontsize=13)\nax1.set_title('All weights', fontsize= 14)\nax2.set_title('Non-zero weights', fontsize= 14)\nax1.set_xlabel('')\nax2.set_xlabel('');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=df, x='resp',y='weight', color= 'blue', alpha=.3)\nplt.title('Resp vs Weight\\ncorrelation={}'.format(round(df.weight.corr(df.resp),4)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can see that weight is not linearly correlated with Resp but it's clear that higher weight are only associated with low Resp values"},{"metadata":{},"cell_type":"markdown","source":"<a id='features'></a>\n# Features data analysis"},{"metadata":{},"cell_type":"markdown","source":"### Loading the features csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_f =  pd.read_csv('../input/jane-street-market-prediction/features.csv')\ndf_f.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df_f.set_index('feature').T.sum(), title='Number of tags for each feature')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces( showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='nulls'></a>\n## **Exploring the Null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(x = df.isnull().sum().index,y= df.isnull().sum().values,title= 'Number of Null values')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.yaxis. dtick = 100000\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.layout.xaxis.showgrid = True\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding out the features with missing values more than 10 %"},{"metadata":{"trusted":true},"cell_type":"code","source":"0.1 * len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls = df.isnull().sum()\nnulls_list = list(nulls[(nulls >239049)].index)\nnulls_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Looking into the relationship between these features since there is kind of pattern in the number of null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[nulls_list].corr().style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since the number of null values in these columns are huge (larger than one quarter of a million!) we will be dropping nulls with more than 10% null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=nulls_list,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### now concerning the remaining nulls we will look firstly to the coefficient of variation"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.iloc[:,7:-2].std() / df.iloc[:,7:-2].mean()).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### the coefficient of variation seems unreliable due to the value of the mean being near to zero"},{"metadata":{},"cell_type":"markdown","source":"### Now we can take a bird's-eye view  of features distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[:,7:-2].hist(bins=100,figsize=(20,74),layout=(29,4));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **The following code will make a grid of horizontal box plot with the mean ploted too to get a comprehensive solid understanding of the features distributions**\n> #### please note that we used customized 0.1%:99.9% whisker to show extreme outliers since the data is strongly centered."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,80))\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\nfeatstr = [i for i in df.columns[7:-2]]\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= df[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.axvline(df[featstr[counter]].mean(),color= 'darkorange')\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=14)\n        counter += 1\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that there is a lot of outliers affecting the distribution of each feature.\n### Also since the majority of values are heavily centerd around the mean, we will fill null values using the mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mean().reset_index().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(df.mean(axis=0),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='growth'></a>\n## Features growth "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,24))\ngrid =  gridspec.GridSpec(7,5,figure=fig,hspace=.3,wspace=.3)\ncounter = 0\nf = featstr[1:36]\nfor i in range(7):\n    for j in range(5):\n        subf = fig.add_subplot(grid[i, j]);\n        cumsum = df[f[counter]].cumsum()\n        subf.plot(cumsum,color= 'darkblue', alpha=.7);\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{}'.format(f[counter]),fontsize=14)\n        counter += 1\n        del cumsum\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,24))\ngrid =  gridspec.GridSpec(7,5,figure=fig,hspace=.3,wspace=.3)\ncounter = 0\nf = featstr[36:71]\nfor i in range(7):\n    for j in range(5):\n        subf = fig.add_subplot(grid[i, j]);\n        cumsum = df[f[counter]].cumsum()\n        subf.plot(cumsum,color= 'darkblue', alpha=.7);\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{}'.format(f[counter]),fontsize=14)\n        counter += 1\n        del cumsum\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,24))\ngrid =  gridspec.GridSpec(7,5,figure=fig,hspace=.3,wspace=.3)\ncounter = 0\nf = featstr[71:106]\nfor i in range(7):\n    for j in range(5):\n        subf = fig.add_subplot(grid[i, j]);\n        cumsum = df[f[counter]].cumsum()\n        subf.plot(cumsum,color= 'darkblue', alpha=.7);\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{}'.format(f[counter]),fontsize=14)\n        counter += 1\n        del cumsum\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,6))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.3,wspace=.3)\ncounter = 0\nf = featstr[106:]\nfor i in range(2):\n    for j in range(5):\n        subf = fig.add_subplot(grid[i, j]);\n        cumsum = df[f[counter]].cumsum()\n        subf.plot(cumsum,color= 'darkblue', alpha=.7);\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{}'.format(f[counter]),fontsize=14)\n        counter += 1\n        del cumsum\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Many features seem to be linearly growing but some features like 81, 82, and 83 are actually decreasing, and there are some features that fluctuate "},{"metadata":{},"cell_type":"markdown","source":"<a id='multicollinearity'></a>\n## Correlation between features"},{"metadata":{},"cell_type":"markdown","source":"#### First we make a correlation dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.iloc[:,7:-2].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting a heatmap for features correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,12))\nax = plt.subplot(1,1,1)\nsns.heatmap(corr,ax= ax, cmap='coolwarm');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we check pearson correlation coefficient values for all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">### **It looks like there is a lot of multicollinearity between features and also it looks like there is a pattern of couples in the features space and this pattern is kinda broke at some features  like feature_41**"},{"metadata":{"trusted":true},"cell_type":"code","source":"featstr2 = [ i for i in featstr if i not in ['feature_41','feature_64']]\nlen(featstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(22,44))\ngrid =  gridspec.GridSpec(12,5,figure=fig,hspace=.5,wspace=.2)\ncounter = 1\nfor i in range(12):\n    for j in range(5):\n        if counter == 113:\n            break\n        subf = fig.add_subplot(grid[i, j]);\n        sns.scatterplot(x= df[featstr2[counter]], y = df[featstr2[counter+1]], ax= subf);\n        cor = round(df[featstr2[counter]].corr(df[featstr2[counter+1]]) * 100,2)\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{} & {}\\nCorrelation = {}%'.format(featstr2[counter],featstr2[counter+1],cor),fontsize=14)\n        counter += 2\n        gc.collect();  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### As expected since these are finance related features a lot of features are highly correlate with each other"},{"metadata":{},"cell_type":"markdown","source":"### Now we explore highly correlated groups of features"},{"metadata":{},"cell_type":"markdown","source":"#### We start off with featrues: **[feature_19, feature_20, feature_21, feature_22, feature_23, feature_24, feature_25, feature_26, feature_29]** since there is kind of multicollinearity cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6)) \nsns.heatmap(df[featstr2[15:23]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[featstr2[15:23]],corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Despite the fact that Pearson coefficients of correlation are really high between these features the relationships are not completely linear, also it can be noticed that the outliers affect the shape of scatter plots."},{"metadata":{},"cell_type":"markdown","source":"### Now we check the correlation again but between other group "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6)) \nsns.heatmap(df[featstr2[23:31]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[featstr2[23:31]],corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### things looks like kinda the same with the other cluster, it  also worth mentioning that both of these features clusters are negatively correlated with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6)) \nsns.heatmap(df[featstr2[15:31]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> <a id='outlier'></a>\n## Outliers"},{"metadata":{},"cell_type":"markdown","source":"#### First we take a look at the mean of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].mean(), title='Features mean values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].max(), title='Features Max Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].min(), title='Features Min Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2,ax3)= plt.subplots(3,1,figsize=(10,12))\nplt.subplots_adjust(hspace=.3)\nsns.distplot(df[featstr].max(),ax= ax1 )\nsns.distplot(df[featstr].min(),ax= ax2)\nsns.distplot(df[featstr].mean(),ax= ax3)\nfig.suptitle('distribution of mean max and min for features',fontsize=16)\nax1.set_title('distribution  of features max values',fontsize=14)\nax1.text(.82,.56,'std = {}'.format(round(df[featstr].max().std(),2)),transform=ax1.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax2.set_title('distribution  of features min values',fontsize=14)\nax2.text(.82,.56,'std = {}'.format(round(df[featstr].min().std(),2)),transform=ax2.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax3.set_title('distribution  of features mean values',fontsize=14)\nax3.text(.82,.56,'std = {}'.format(round(df[featstr].mean().std(),2)),transform=ax3.transAxes, verticalalignment='top',bbox=props,fontsize=12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can see that despite the fact the mean values are not that different from each other the min and max values are very deviated with highly skewed distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we do more statistically oriented exploring to outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in featstr[1:]:\n    print('{}\\n0.1%:99.9% are between: {}\\nmax: {}\\nmin: {}\\n75% are under: {}'.format(i,np.percentile(df[i],(.1,99.9)), df[i].max(),df[i].min(),np.percentile(df[i],75)),\n         '\\n===============================')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[(df.feature_56== df.feature_56.max())|(df.feature_57== df.feature_57.max())|(df.feature_58== df.feature_58.max()) | (df.feature_59== df.feature_59.max())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It can be inferred that the dataset has extreme outliers it also worth mentioning that some of outliers are accompanied with large values from neighbor columns which is a result of high multicollinearity between features"},{"metadata":{},"cell_type":"markdown","source":"### Now we will make a bold move by removing these extreme outliers that are above 99.9% of feature data"},{"metadata":{},"cell_type":"markdown","source":"#### To avoid removing more data while looping through the data set we will make a list of 99.9% mark for each and every single feature\n#### We will also create a list for negative outliers values \"using .1 % mark\" to be explored later "},{"metadata":{"trusted":true},"cell_type":"code","source":"n999 = [ np.percentile(df[i],99.9) for i in featstr[1:]]\nn001 = [ np.percentile(df[i],.1) for i in featstr[1:]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we iterate over the dataset to remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in enumerate(featstr[1:]):\n    df = df[df[j] < n999[i]]\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the ratio of the data lost in removing the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"str(round(((org_len - len(df))/org_len)*100,2))+'%'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].max(), title='Features Max Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we make a boxplot grid again but with customized .1% : 99.9% whiskers."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,80))\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= df[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=14)\n        counter += 1\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### The dataset still has many obvious outliers especially negative values since we only removed positive outliers so features which used to have symmetrical outliers now have some kind of left skewed outliers.\n>### comparing both of the boxplots (before and after removing positive outliers) we can notice features from feature_3 to feature_40 which used to have symmetrical outliers now changed to have extreme negative outliers after trimming\n\n"},{"metadata":{},"cell_type":"markdown","source":"### now we do some manual outlier trimming to these features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in zip(featstr[1:][2:34],n001[2:34]):\n    df = df[df[i] > j]\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### now we find the total observations data loss in cleaning so far"},{"metadata":{"trusted":true},"cell_type":"code","source":"str(round(((org_len - len(df))/org_len)*100,2))+'%'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we Check the histograms of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[:,7:-2].hist(bins=100,figsize=(20,74),layout=(29,4));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can now see the following:\n- the histograms of features now have way less outliers and more formal distribution\n- We can also see that some features like 1, 2, 85, 87, 88 and 91 have many values falling under 0\n- Some features like 49, 50, 51, 55, 56, 57, 58, and 59 still have many positive outliers"},{"metadata":{},"cell_type":"markdown","source":"### Now we explore the relation between features and resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"respcorr =  pd.Series([ df.resp.corr(df[i]) for i in featstr],index=featstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(respcorr,color = respcorr, color_continuous_scale=['red','blue'], title= 'Features Correlation with Resp')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can see that features are not really correlated to Resp"},{"metadata":{},"cell_type":"markdown","source":"### Features and Weight"},{"metadata":{},"cell_type":"markdown","source":"#### First we make a dataframe of weight correlation with feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"wecorr = []\nfor i in featstr:\n    wecorr.append((df[df.weight != 0].weight.corr(df[df.weight != 0][i])))\nwecorr = pd.DataFrame(wecorr,index=featstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wecorr.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wecorr[wecorr[0] == wecorr[0].min()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wecorr[wecorr[0] == wecorr[0].max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(wecorr,title= 'Features Correlation with Weight (no including zero weights)')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.update_layout(showlegend=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(df[df.weight != 0].weight,df[df.weight != 0].feature_51, color = 'blue', alpha=.4)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_51',fontsize=14)\nplt.title('Feature_51 vs Weight\\nCorrelation = {}%'.format(round(df[df.weight != 0].weight.corr(df[df.weight != 0].feature_51),4)*100),fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It seems that weight is highly correlated to feature 51"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(df[df.weight != 0].weight,df[df.weight != 0].feature_126, color = 'blue', alpha=.4)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_126',fontsize=14)\nplt.title('Feature_126 vs Weight\\nCorrelation{}%'.format(round(df[df.weight != 0].weight.corr(df[df.weight != 0].feature_126),4)*100),fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### While there is some kind negative correlation between weight and feature 126 the relation seem to be weak"},{"metadata":{},"cell_type":"markdown","source":"<a id='f0'></a>\n## **Feature 0**"},{"metadata":{},"cell_type":"markdown","source":"#### Finding the unique values of Feature_0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.feature_0.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5)) \ndf.feature_0.value_counts().plot.bar(color='darkblue',alpha=.6,width=.5)\nplt.title('Feature_0',fontsize=18) \nplt.xticks(rotation=0,fontsize=14) ;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Feature_0 seem to be some kind of binary feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6)) \nsns.countplot(data=df, x='feature_0', hue='action',palette='viridis')\nplt.legend(ncol=1, fontsize=12, loc=3,title= 'Action',title_fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=13)\nplt.xlabel('Feature 0',fontsize=12)\nplt.title('Feature 0 and Action', fontsize=18)\nplt.ylabel('')\nplt.xlim(-1,2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### seems like there is not an obvious relation between feature_0 and resp being negative or positive"},{"metadata":{},"cell_type":"markdown","source":"# <font color='green'>Work in progress  █████████▒▒</font> "},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Upvote this notebook and Michelangelo will get his favorite pizza</center></h3>\n\n![](http://i.imgflip.com/1ydu71.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}